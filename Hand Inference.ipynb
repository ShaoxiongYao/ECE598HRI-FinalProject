{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbaf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import scipy.optimize as sopt\n",
    "from klampt.math import se3,so3\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "from create_point_cloud import load_whole_point_cloud, load_point_cloud,get_masked_point_cloud\n",
    "import scipy\n",
    "import torch\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from collections import deque\n",
    "from queue import Queue\n",
    "from threading import Event, Lock, Thread\n",
    "\n",
    "import cv2\n",
    "\n",
    "from mmpose.apis import (get_track_id, inference_top_down_pose_model,\n",
    "                         init_pose_model, vis_pose_result)\n",
    "from mmpose.core import apply_bugeye_effect, apply_sunglasses_effect\n",
    "from mmpose.utils import StopWatch\n",
    "\n",
    "try:\n",
    "    from mmdet.apis import inference_detector, init_detector\n",
    "    has_mmdet = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    has_mmdet = False\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    psutil_proc = psutil.Process()\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    psutil_proc = None\n",
    "from torch import nn\n",
    "    \n",
    "from utils import get_proper_image_paths, strip_dataset_parent_folder,get_aligned_dataset,find_hand_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117706c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMPOSE_DIR = '/home/motion/Joao/classes/hri_kdc/final_project/mmpose'\n",
    "det_config = '{}/demo/mmdetection_cfg/ssdlite_mobilenetv2_scratch_600e_coco.py'.format(MMPOSE_DIR)\n",
    "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/ssd/ssdlite_mobilenetv2_scratch_600e_coco/ssdlite_mobilenetv2_scratch_600e_coco_20210629_110627-974d9307.pth'\n",
    "device = 'cuda:0'\n",
    "enable_human_pose = 1\n",
    "human_pose_config = '{}/configs/wholebody/2d_kpt_sview_rgb_img/topdown_heatmap/coco-wholebody/vipnas_res50_coco_wholebody_256x192_dark.py'.format(MMPOSE_DIR)\n",
    "human_pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/vipnas/vipnas_res50_wholebody_256x192_dark-67c0ce35_20211112.pth'\n",
    "human_det_ids = [1]\n",
    "buffer_size = 1\n",
    "display_delay = 0\n",
    "assert has_mmdet, 'Please install mmdet to run the demo.'\n",
    "assert det_config is not None\n",
    "assert det_checkpoint is not None\n",
    "\n",
    "# build detection model\n",
    "det_model = init_detector(\n",
    "    det_config, det_checkpoint, device=device.lower())\n",
    "\n",
    "# build pose models\n",
    "pose_model_list = []\n",
    "if enable_human_pose:\n",
    "    pose_model = init_pose_model(\n",
    "        human_pose_config,\n",
    "        human_pose_checkpoint,\n",
    "        device=device.lower())\n",
    "    model_info = {\n",
    "        'name': 'HumanPose',\n",
    "        'model': pose_model,\n",
    "        'cat_ids': human_det_ids,\n",
    "        'bbox_color': (148, 139, 255),\n",
    "    }\n",
    "    pose_model_list.append(model_info)\n",
    "\n",
    "\n",
    "# store pose history for pose tracking\n",
    "pose_history_list = []\n",
    "\n",
    "# for _ in range(len(pose_model_list)):\n",
    "#     pose_history_list.append({'pose_results_last': [], 'next_id': 0})\n",
    "    \n",
    "# datasets_dir = '/home/motion/data/ECE598/our_dataset'\n",
    "# scenes = sorted(glob(datasets_dir + '/*'))\n",
    "\n",
    "# dataset_folder = scenes[0]\n",
    "\n",
    "# clean_df =  get_aligned_dataset(dataset_folder,master_camera = 'cam_torso_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = cv2.imread(clean_df.loc[0,'cam_torso_color'], cv2.IMREAD_COLOR)\n",
    "# dataset_name = pose_model.cfg.data['test']['type']\n",
    "\n",
    "# res,outputs = inference_top_down_pose_model(\n",
    "#             pose_model,\n",
    "#             frame,\n",
    "#             format='xyxy',\n",
    "#             dataset=dataset_name,return_heatmap = True)\n",
    "# heatmap = outputs[0]['heatmap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# points = get_masked_point_cloud(clean_df.loc[0,'cam_torso_depth'],'realsense_torso')\n",
    "# points.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4054b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hands_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, master_df,pose_model,history_length = 30,pred_horizon = 15):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.master_df = master_df\n",
    "        self.history_length = history_length\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.max_idx = self.master_df.shape[0]\n",
    "        mask = np.zeros(133)\n",
    "        mask[:] = False\n",
    "        mask[5:24] = True\n",
    "        mask[91:] = True\n",
    "        self.pose_model = pose_model\n",
    "        self.mask = mask\n",
    "        feature_lenght = 10800 + 187392\n",
    "    def __len__(self):\n",
    "        return self.master_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        imgs_idx = np.arange(idx,idx-self.history_length,-5)\n",
    "        imgs_idx = np.clip(imgs_idx,0,self.max_idx)\n",
    "        gt_idx = np.clip([idx+self.pred_horizon],0,self.max_idx)[0]\n",
    "        features = []\n",
    "        for img_idx in imgs_idx:\n",
    "#             color = cv2.imread(self.master_df.loc[img_idx,'cam_torso_color'], cv2.IMREAD_COLOR)\n",
    "            color = self.master_df.loc[img_idx,'pre_processed_torso_heatmap']\n",
    "            point = get_masked_point_cloud(self.master_df.loc[img_idx,'cam_torso_depth'],'realsense_torso')\n",
    "            feature = np.concatenate((point.flatten(),color.flatten()))\n",
    "            features.append(feature)\n",
    "#         colors = np.array(colors)\n",
    "        features = np.array(features)\n",
    "        gt_left = self.master_df.loc[gt_idx,'gt_left_hand']\n",
    "        gt_right = self.master_df.loc[gt_idx,'gt_right_hand']\n",
    "        gt = np.concatenate((gt_left,gt_right))\n",
    "#         print(gt)\n",
    "#         img_name = os.path.join(self.root_dir,\n",
    "#                                 self.landmarks_frame.iloc[idx, 0])\n",
    "#         image = io.imread(img_name)\n",
    "#         landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "#         landmarks = np.array([landmarks])\n",
    "#         landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'data': features, 'gt_pose': gt}\n",
    "\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5292211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = pd.read_pickle('./first_ground_truth_alice_no_sword1.pkl')\n",
    "# clean_df = get_proper_image_paths(raw_df,'/home/motion/data/ECE598/our_dataset')\n",
    "clean_df = pd.read_pickle('alice_no_sword_pre_processed.pkl')\n",
    "dataset = Hands_Dataset(clean_df,pose_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2126490",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=16, shuffle=False,num_workers = 4,prefetch_factor= 8,persistent_workers=  True)\n",
    "\n",
    "for i in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,pose_model):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.pose_model = pose_model\n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    def preprocess_input(self,x):\n",
    "        res,outputs = inference_top_down_pose_model(\n",
    "        self.pose_model,\n",
    "        frame,\n",
    "        format='xyxy',\n",
    "        dataset=dataset_name,return_heatmap = True)\n",
    "        heatmap = outputs[0]['heatmap'][0]\n",
    "        heatmap = heatmap[self.mask,:,:].flatten()\n",
    "    def extract_heatmap(self,img_file):\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "mask = np.zeros(133).astype(bool)\n",
    "mask[:] = False\n",
    "mask[5:24] = True\n",
    "mask[91:] = True\n",
    "hm = []\n",
    "for i in tqdm(clean_df.cam_torso_color):  \n",
    "    try:\n",
    "        inference_top_down_pose_model(\n",
    "                pose_model,\n",
    "                i,\n",
    "                format='xyxy',\n",
    "                dataset=dataset_name,return_heatmap = True)\n",
    "        heatmap = outputs[0]['heatmap'][0]\n",
    "        heatmap = heatmap[mask,:,:].flatten()\n",
    "        hm.append(heatmap)\n",
    "    except:\n",
    "        hm.append(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73755b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['pre_processed_torso_heatmap'] = hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d13421",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_pickle('alice_no_sword_pre_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a665f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
